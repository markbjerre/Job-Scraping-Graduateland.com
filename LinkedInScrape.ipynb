{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########################\n",
    "## Mark Bjerregaard     #\n",
    "## MARKBJ@UMICH.EDU     #\n",
    "#########################\n",
    "'''\n",
    "Linkedin job scrape script\n",
    "Scrapes jobs using selenium and chromedriver available here: https://chromedriver.chromium.org/\n",
    "\n",
    "Steps:\n",
    "1. Enter Linkedin log-in page\n",
    "2. Spot the cookies pop-up and accept cookies\n",
    "3. Fill E-Mail Adress and Password areas and click login\n",
    "4. Click on the jobs from the section above\n",
    "5. Search for job positions Data Analyst \n",
    "6. Scroll till end of page, collecting links on the way\n",
    "7. Go to the next page when it is the end of the page while keep collecting links\n",
    "8. After all links are collected, go to each link\n",
    "9. Click the see more button to expand the job description text\n",
    "10. Scrape the desired data\n",
    "\n",
    "\n",
    "Items to be scraped \n",
    "1. Job title \n",
    "2. Company name\n",
    "3. Company location\n",
    "4. job description\n",
    "5. work method (hybrid, remote, on-site)\n",
    "6. Post date\n",
    "'''\n",
    "''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping job links, page: 1\n",
      "scraping job links, page: 2\n",
      "scraping job links, page: 3\n",
      "scraping job links, page: 4\n",
      "scraping job links, page: 5\n",
      "scraping job links, page: 6\n",
      "scraping job links, page: 7\n",
      "scraping job links, page: 8\n",
      "scraping job links, page: 9\n",
      "scraping job links, page: 10\n",
      "scraping job links, page: 11\n",
      "scraping job links, page: 12\n",
      "scraping job links, page: 13\n",
      "scraping job links, page: 14\n",
      "scraping job links, page: 15\n",
      "scraping job links, page: 16\n",
      "scraping job links, page: 17\n",
      "scraping job links, page: 18\n",
      "scraping job links, page: 19\n",
      "scraping job links, page: 20\n",
      "scraping job links, page: 21\n",
      "scraping job links, page: 22\n",
      "scraping job links, page: 23\n",
      "scraping job links, page: 24\n",
      "scraping job links, page: 25\n",
      "scraping job links, page: 26\n",
      "scraping job links, page: 27\n",
      "scraping job links, page: 28\n",
      "scraping job links, page: 29\n",
      "scraping job links, page: 30\n",
      "scraping job links, page: 31\n",
      "scraping job links, page: 32\n",
      "scraping job links, page: 33\n",
      "scraping job links, page: 34\n",
      "scraping job links, page: 35\n",
      "scraping job links, page: 36\n",
      "scraping job links, page: 37\n",
      "scraping job links, page: 38\n",
      "scraping job links, page: 39\n",
      "scraping job links, page: 40\n",
      "scraping job links, page: 41\n",
      "scraping job links, page: 42\n",
      "scraping job links, page: 43\n",
      "scraping job links, page: 44\n",
      "scraping job links, page: 45\n",
      "scraping job links, page: 46\n",
      "scraping job links, page: 47\n",
      "scraping job links, page: 48\n",
      "scraping job links, page: 49\n",
      "scraping job links, page: 50\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import json\n",
    "\n",
    "page_iterations = 50\n",
    "search_area = 'Copenhagen Metropolitan Area'\n",
    "search_position = 'Data Analyst'\n",
    "\n",
    "def open_cache(cache_name):\n",
    "    '''\n",
    "    opens the cache file if it exists and loads the JSON into a dictionary, which it then returns\n",
    "    If the cahce file doesn't exist, creates a new chace dictionary\n",
    "    Parameters\n",
    "    ---\n",
    "    None \n",
    "    Returns\n",
    "    ---\n",
    "    The opened cache\n",
    "    '''\n",
    "    try:\n",
    "        cache_file = open(cache_name, 'r')\n",
    "        cache_contents = cache_file.read()\n",
    "        cache_dict = json.loads(cache_contents)\n",
    "        cache_file.close()\n",
    "    except:\n",
    "        cache_dict = {}\n",
    "    return cache_dict\n",
    "\n",
    "def save_cache(cache_dict, cache_name):\n",
    "    '''\n",
    "    saves the current state of the cache to disk\n",
    "    Parameters\n",
    "    ---\n",
    "    cache_dict: dict\n",
    "        the dictionary to save\n",
    "    Returns\n",
    "    ---\n",
    "    None\n",
    "    '''\n",
    "    dumped_json_cache = json.dumps(cache_dict, indent=4)\n",
    "    fw = open(cache_name,'w')\n",
    "    fw.write(dumped_json_cache)\n",
    "    fw.close()\n",
    "\n",
    "def job_scrape(driver):\n",
    "    '''\n",
    "    Function for scraping page of job links\n",
    "    Input: chromedriver (current webpage)\n",
    "    output: list of job site links\n",
    "    ''' \n",
    "    links_cache = 'link_cache.json'\n",
    "    link_dict = open_cache(links_cache)\n",
    "\n",
    "    link_count = max(list(link_dict.values()))\n",
    "    jobs_list = driver.find_elements(By.CLASS_NAME, 'job-box')\n",
    "    for job in jobs_list:\n",
    "        link = job.get_attribute('href')\n",
    "        if not link_dict.get(link):\n",
    "            link_dict[job.get_attribute('href')] = link_count + 1\n",
    "            link_count += 1 \n",
    "\n",
    "    save_cache(link_dict, links_cache)\n",
    "\n",
    "    return link_dict\n",
    "\n",
    "#Driver's path\n",
    "options = Options()\n",
    "options.add_argument('start-maximized')\n",
    "driver = webdriver.Chrome(service =Service('chromedriver.exe'), options=options)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Open linkedin page\n",
    "driver.get('https://graduateland.com/jobs')\n",
    "time.sleep(2)\n",
    "\n",
    "# Filtering on area\n",
    "driver.find_element(By.XPATH, '//*[@id=\"search-filters\"]/div[46]/div/div[1]/input').send_keys(search_area)\n",
    "driver.find_element(By.XPATH, '//*[@id=\"search-filters\"]/div[46]/div/div[1]/input').click()\n",
    "driver.find_element(By.XPATH, '//*[@id=\"search-filters\"]/div[46]/div/div[2]/label[2]').click()\n",
    "\n",
    "# Entering position filter keywords\n",
    "driver.find_element(By.XPATH, '//*[@id=\"job-search-form\"]/div[1]/div[1]/div[1]/input').send_keys(search_position, Keys.ENTER)\n",
    "time.sleep(3)\n",
    "\n",
    "#initializing scrape loop at page 1\n",
    "page = 1 \n",
    "print('scraping job links, page:', page)\n",
    "links = job_scrape(driver)\n",
    "\n",
    "#pages we want to scrape\n",
    "\n",
    "\n",
    "for i in range(1, page_iterations):\n",
    "    if i < 3:\n",
    "        index = i * 2 \n",
    "    elif i < 6:\n",
    "        index = i + 2\n",
    "    else:\n",
    "        i = 7\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"timeline\"]/div[3]/div/a[{}]'.format(index)).click()\n",
    "    page += 1\n",
    "    print('scraping job links, page:', page)\n",
    "    time.sleep(3)\n",
    "    links = job_scrape(driver)\n",
    "    if i % 20 == 0:\n",
    "        time.sleep(60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "acb118ada14cafd3b3da3ef21dccf95768441697cf5d12d40daa79f849726a2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
