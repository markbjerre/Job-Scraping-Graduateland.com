{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "## Mark Bjerregaard     #\n",
    "## MARKBJ@UMICH.EDU     #\n",
    "#########################\n",
    "\n",
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "'''\n",
    "Linkedin job scrape script\n",
    "Scrapes jobs using selenium and chromedriver available here: https://chromedriver.chromium.org/\n",
    "\n",
    "Steps:\n",
    "1. Enter Linkedin log-in page\n",
    "2. Spot the cookies pop-up and accept cookies\n",
    "3. Fill E-Mail Adress and Password areas and click login\n",
    "4. Click on the jobs from the section above\n",
    "5. Search for job positions Data Analyst \n",
    "6. Scroll till end of page, collecting links on the way\n",
    "7. Go to the next page when it is the end of the page while keep collecting links\n",
    "8. After all links are collected, go to each link\n",
    "9. Click the see more button to expand the job description text\n",
    "10. Scrape the desired data\n",
    "\n",
    "\n",
    "Items to be scraped \n",
    "1. Job title \n",
    "2. Company name\n",
    "3. Company location\n",
    "4. job description\n",
    "5. work method (hybrid, remote, on-site)\n",
    "6. Post date\n",
    "'''\n",
    "''\n",
    "\n",
    "def open_cache(cache_name):\n",
    "    '''\n",
    "    opens the cache file if it exists and loads the JSON into a dictionary, which it then returns\n",
    "    If the cahce file doesn't exist, creates a new chace dictionary\n",
    "    Parameters\n",
    "    ---\n",
    "    None \n",
    "    Returns\n",
    "    ---\n",
    "    The opened cache\n",
    "    '''\n",
    "    try:\n",
    "        cache_file = open(cache_name, 'r')\n",
    "        cache_contents = cache_file.read()\n",
    "        cache_dict = json.loads(cache_contents)\n",
    "        cache_file.close()\n",
    "    except:\n",
    "        cache_dict = {}\n",
    "    return cache_dict\n",
    "\n",
    "def save_cache(cache_dict, cache_name):\n",
    "    '''\n",
    "    saves the current state of the cache to disk\n",
    "    Parameters\n",
    "    ---\n",
    "    cache_dict: dict\n",
    "        the dictionary to save\n",
    "    Returns\n",
    "    ---\n",
    "    None\n",
    "    '''\n",
    "    dumped_json_cache = json.dumps(cache_dict, indent=4)\n",
    "    fw = open(cache_name,'w')\n",
    "    fw.write(dumped_json_cache)\n",
    "    fw.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import json\n",
    "\n",
    "\n",
    "page_iterations = 100\n",
    "search_area = 'Copenhagen Metropolitan Area'\n",
    "search_position = 'Data Analyst'\n",
    "\n",
    "def open_cache(cache_name):\n",
    "    '''\n",
    "    opens the cache file if it exists and loads the JSON into a dictionary, which it then returns\n",
    "    If the cahce file doesn't exist, creates a new chace dictionary\n",
    "    Parameters\n",
    "    ---\n",
    "    None \n",
    "    Returns\n",
    "    ---\n",
    "    The opened cache\n",
    "    '''\n",
    "    try:\n",
    "        cache_file = open(cache_name, 'r')\n",
    "        cache_contents = cache_file.read()\n",
    "        cache_dict = json.loads(cache_contents)\n",
    "        cache_file.close()\n",
    "    except:\n",
    "        cache_dict = {}\n",
    "    return cache_dict\n",
    "\n",
    "def save_cache(cache_dict, cache_name):\n",
    "    '''\n",
    "    saves the current state of the cache to disk\n",
    "    Parameters\n",
    "    ---\n",
    "    cache_dict: dict\n",
    "        the dictionary to save\n",
    "    Returns\n",
    "    ---\n",
    "    None\n",
    "    '''\n",
    "    dumped_json_cache = json.dumps(cache_dict, indent=4)\n",
    "    fw = open(cache_name,'w')\n",
    "    fw.write(dumped_json_cache)\n",
    "    fw.close()\n",
    "\n",
    "def job_scrape(driver):\n",
    "    '''\n",
    "    Function for scraping page of job links\n",
    "    Input: chromedriver (current webpage)\n",
    "    output: list of job site links\n",
    "    ''' \n",
    "    links_cache = 'link_cache.json'\n",
    "    link_dict = open_cache(links_cache)\n",
    "\n",
    "    link_count = max(list(link_dict.values()))\n",
    "    jobs_list = driver.find_elements(By.CLASS_NAME, 'job-box')\n",
    "    for job in jobs_list:\n",
    "        link = job.get_attribute('href')\n",
    "        if not link_dict.get(link):\n",
    "            link_dict[job.get_attribute('href')] = link_count + 1\n",
    "            link_count += 1 \n",
    "\n",
    "    save_cache(link_dict, links_cache)\n",
    "\n",
    "    return link_dict\n",
    "\n",
    "# Chrome driver setup & browser open\n",
    "options = Options()\n",
    "options.add_argument('start-maximized')\n",
    "driver = webdriver.Chrome(service =Service('chromedriver.exe'), options=options)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Open page\n",
    "driver.get('https://graduateland.com/jobs')\n",
    "time.sleep(2)\n",
    "\n",
    "# Filtering on area\n",
    "driver.find_element(By.XPATH, '//*[@id=\"search-filters\"]/div[46]/div/div[1]/input').send_keys(search_area)\n",
    "driver.find_element(By.XPATH, '//*[@id=\"search-filters\"]/div[46]/div/div[1]/input').click()\n",
    "driver.find_element(By.XPATH, '//*[@id=\"search-filters\"]/div[46]/div/div[2]/label[2]').click()\n",
    "\n",
    "# Entering position filter keywords\n",
    "driver.find_element(By.XPATH, '//*[@id=\"job-search-form\"]/div[1]/div[1]/div[1]/input').send_keys(search_position, Keys.ENTER)\n",
    "time.sleep(3)\n",
    "\n",
    "#initializing scrape loop at page 1\n",
    "page = 1 \n",
    "print('scraping job links, page:', page)\n",
    "links = job_scrape(driver)\n",
    "\n",
    "# Page iterator\n",
    "for i in range(1, page_iterations):\n",
    "    # handling XPATH variations based on page numbers\n",
    "    if i < 3:\n",
    "        index = i * 2 \n",
    "    elif i < 6:\n",
    "        index = i + 2\n",
    "    else:\n",
    "        i = 7\n",
    "    # Clicking next page button\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"timeline\"]/div[3]/div/a[{}]'.format(index)).click()\n",
    "    page += 1\n",
    "    print('scraping job links, page:', page)\n",
    "    time.sleep(3)\n",
    "    # Scraping job post links from page\n",
    "    links = job_scrape(driver)\n",
    "    if i % 20 == 0:\n",
    "        time.sleep(60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Offline_test(soup, source):\n",
    "    #returns True of job is offline\n",
    "    offline = soup.find('section', class_='job-offline')\n",
    "    if offline == None:\n",
    "        return False\n",
    "    else:\n",
    "        links_cache = 'link_cache.json'\n",
    "        link_dict = open_cache(links_cache)\n",
    "        del link_dict[source]\n",
    "        save_cache(link_dict, links_cache)\n",
    "        return True\n",
    "\n",
    "def Scraper(url):\n",
    "    \n",
    "    def attribute_extract(soup):\n",
    "        attributes = soup.find('div', class_='content-description')\n",
    "        attributes = attributes.find_all('p')\n",
    "        last = ''\n",
    "        attributes_list = [i.text.strip() for i in attributes if i.text != None]\n",
    "        idx = 0\n",
    "        for i in attributes_list:\n",
    "            new_elem = re.sub('  ', '', i)\n",
    "            new_elem = re.sub('\\\\n\\\\n\\\\n','___',new_elem)\n",
    "            new_elem = new_elem.split('___')\n",
    "            if len(new_elem) != 1:\n",
    "                new_elem = [re.sub('\\\\n', '', i) for i in new_elem]\n",
    "                attributes_list[idx] = new_elem\n",
    "            else:\n",
    "                new_elem = re.sub('\\\\n',' ',new_elem[0])\n",
    "                attributes_list[idx] = new_elem\n",
    "            idx += 1 \n",
    "        return attributes_list\n",
    "\n",
    "\n",
    "    vertex_cache = 'vertex_cache.json'\n",
    "    vert_dict = open_cache(vertex_cache)\n",
    "    if url in vert_dict:\n",
    "        attr_dict = vert_dict[url]\n",
    "        return attr_dict\n",
    "    else:\n",
    "        attr_dict = {}\n",
    "\n",
    "\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "\n",
    "    # Title \n",
    "    title_ = soup.find('div',class_='job-title').find('h1').text\n",
    "    attr_dict['title'] = title_\n",
    "\n",
    "    # Job description\n",
    "    job_desc_ = soup.find('article', class_='box-item job-content').text\n",
    "    attr_dict['job_desc'] = job_desc_\n",
    "\n",
    "\n",
    "    headline = soup.find('div', class_='headline')\n",
    "    # Company\n",
    "        \n",
    "    company_ = headline.find('h2').text.strip()\n",
    "    attr_dict['company'] = company_\n",
    "\n",
    "    # Industry\n",
    "    industry_ = headline.find_all('p')[0].text.strip()\n",
    "    attr_dict['industry'] = industry_\n",
    "    \n",
    "    # Followers\n",
    "    followers_ = headline.find_all('p')[1].text.strip()\n",
    "    followers_ = re.sub('[^0-9]', '', followers_)\n",
    "    attr_dict['followers'] = followers_\n",
    "\n",
    "    #logo\n",
    "    try:\n",
    "        logo_soup = soup.find('div',class_='company-item')\n",
    "        logo_ = logo_soup.find('img')['src']\n",
    "        attr_dict['logo'] = logo_ \n",
    "    except:\n",
    "        attr_dict['logo'] = None\n",
    "\n",
    "    # Attributes (location, category, job_type, skills, language)\n",
    "    attributes = attribute_extract(soup)\n",
    "    location_ = attributes[0]\n",
    "    category_ = attributes[1]\n",
    "    job_type_ = attributes[2]\n",
    "    skills_ = attributes[3]\n",
    "    language_ = attributes[4]\n",
    "    attr_dict['location'] = location_\n",
    "    attr_dict['category'] = category_\n",
    "    attr_dict['job_type'] = job_type_\n",
    "    attr_dict['skills'] = skills_\n",
    "    attr_dict['language'] = language_\n",
    "\n",
    "    vert_dict[url] = attr_dict\n",
    "    save_cache(vert_dict, vertex_cache)\n",
    "    return attr_dict\n",
    "\n",
    "\n",
    "class Vertex:\n",
    "    def __init__(self, url):\n",
    "\n",
    "        attr_dict = Scraper(url)\n",
    "        self.attr_dict = attr_dict\n",
    "        self.title = attr_dict.get('title')\n",
    "        self.blob = attr_dict.get('job_desc')\n",
    "        self.company = attr_dict.get('company')\n",
    "        self.industry = attr_dict.get('industry')\n",
    "        self.followers = attr_dict.get('followers')\n",
    "        self.logo = attr_dict.get('logo')\n",
    "        self.location = attr_dict.get('location')\n",
    "        self.category = attr_dict.get('category')\n",
    "        self.job_type = attr_dict.get('job_type')\n",
    "        self.skills = attr_dict.get('skills')\n",
    "        self.language_req = attr_dict.get('language')\n",
    "        self.day_extracted = date.today()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "vert_test = Vertex('https://graduateland.com/job/51447115/13')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"title\": \"Student assistant for supporting leaders in their daily tasks at \\u00d8rsted\",\n",
      "    \"job_desc\": \"\\nImagine a future where you use your ability to keep an overview and your coordination skills to support leaders in their daily responsibilities.Join us in this role where you\\u2019ll be responsible for administrative tasks related to our Danish leaders\\u2019 work. You\\u2019ll have onboarding tasks, such as ordering equipment, ID cards, and gifts and you\\u2019ll ensure that all pre-onboarding related tasks are done before hiring date. You\\u2019ll also be responsible for having an overview of all external consultants and, e.g., their extensions and termination. You\\u2019ll furthermore be supporting leaders in conducting yearly recurring people leadership activities.Welcome to ITYou\\u2019ll be part of People Leadership Centre of Excellence (CoE) within IT where you, together with your colleagues, will build and maintain a strong people leadership capability for the IT organisation in our company. In the IT organisation, we work full blown agile and work according to the Scaled Agile Framework. We believe in joint successes and the opportunity to explore, develop, and have fun while working. You\\u2019ll be part of a CoE with five friendly and passionate colleagues where you\\u2019ll be supporting a group of approx. 14 Danish leaders.You\\u2019ll play an important role in:assisting in administrative tasks within the People Leadership CoEfollowing up on extensions, termination, time-registration, and mandatory training for external consultants and escalate if an external is not adhering to rulesparticipating in solving tasks regarding the people leadership backlogfunctioning as contactor support person for all people leaders on people leadership in generalordering ID-cards, equipment, and flowers for all internal and external newcomersbooking placeholders in our leader\\u2019s calendars for known annual wheel activities.To succeed in the role, you:are able to keep an overview and have good coordination skillsthrive in doing administrative taskspreferably have 3-5 years left before graduating with a master\\u2019s degreeare a strong team player with the ability to interact in a larger group with many different tasksare able to keep things simple and separated, as there are many leaders, consultants, and different tasksare slightly familiar with the agile mindset and behaviours.Join a global leader in renewable energy\\u00d8rsted is a growing green energy major and global leader in climate action. With us you\\u2019ll play a part in driving change towards a green energy future. You\\u2019ll grow your talent in a fast-paced and high-growth industry where you have plenty of opportunities to learn and develop through challenging assignments and industry-leading experts. Here, you can perform in a friendly work environment based on trust, respect, and collaboration.Shape the future with usSend your application to us no later than 11 December 2022. We\\u2019ll be conducting interviews on a continuous basis and reserve the right to take down the advert when we have found the right candidate. Please don\\u2019t hesitate to contact Benedikte Eg Moelhave-Kragekjaer, People Leadership Consultant, on (Denmark +45) 99556159 on Tuesdays and Thursdays from 09:00-12:00 CET; Central European Time if you\\u2019d like to know more about the position. If you need to request any adjustments to working practices, working patterns, or the assessment or interview process we're happy to discuss alternative arrangements. Please note that for your application to be taken into consideration, you must submit your application via our online career pages and answer the screening questions relevant for your country.\\n\",\n",
      "    \"company\": \"\\u00d8rsted\",\n",
      "    \"industry\": \"Multiple industries\",\n",
      "    \"followers\": \"4062\",\n",
      "    \"logo\": \"https://d1k976m6pd0u9m.cloudfront.net/public/employer/logo/g43ql0iq55__rsted.png.webp\",\n",
      "    \"location\": \"Gentofte, Denmark\",\n",
      "    \"category\": \"Administration\",\n",
      "    \"job_type\": \"Part-time\",\n",
      "    \"skills\": [\n",
      "        \"Team Player\",\n",
      "        \" Structured\",\n",
      "        \" Coordination\"\n",
      "    ],\n",
      "    \"language\": \"English, Professional\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dict = vert_test.attr_dict\n",
    "pretty = json.dumps(dict, indent=4)\n",
    "print(pretty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student assistant for supporting leaders in their daily tasks at Ørsted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = 'https://graduateland.com/job/51447115/13'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text,'html.parser')\n",
    "test = soup.find('div',class_='job-title').find('h1')\n",
    "\n",
    "print(test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ørsted\n",
      "Multiple industries\n",
      "4062\n",
      "Expires in 5 days\n",
      "https://d1k976m6pd0u9m.cloudfront.net/public/employer/logo/g43ql0iq55__rsted.png.webp\n",
      "Gentofte, Denmark\n",
      "Administration\n",
      "Part-time\n",
      "['Team Player', ' Structured', ' Coordination']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "\n",
    "#BASE_URL = 'https://graduateland.com/job/51447115/13'\n",
    "BASE_URL = 'https://graduateland.com/job/51447115/13'\n",
    "\n",
    "response = requests.get(BASE_URL)\n",
    "soup = BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "##################################################################\n",
    "#job offline test\n",
    "def Offline_test(soup, source):\n",
    "    #returns True of job is offline\n",
    "    offline = soup.find('section', class_='job-offline')\n",
    "    if offline == None:\n",
    "        return False\n",
    "    else:\n",
    "        links_cache = 'link_cache.json'\n",
    "        link_dict = open_cache(links_cache)\n",
    "        del link_dict[source]\n",
    "        save_cache(link_dict, links_cache)\n",
    "        return True\n",
    "\n",
    "\n",
    "# Job description\n",
    "job_desc = soup.find('article', class_='box-item job-content').text\n",
    "# Company\n",
    "headline = soup.find('div', class_='headline') \n",
    "company = headline.find('h2').text.strip()\n",
    "# Industry\n",
    "industry = headline.find_all('p')[0].text.strip()\n",
    "# Followers\n",
    "followers = headline.find_all('p')[1].text.strip()\n",
    "followers = re.sub('[^0-9]', '', followers) \n",
    "#Expiration\n",
    "try:\n",
    "    expiration = soup.find('span', class_= 'text-warning').text.strip()\n",
    "except: \n",
    "    expiration = None\n",
    "\n",
    "#logo\n",
    "logo_soup = soup.find('div',class_='company-item')\n",
    "logo = logo_soup.find('img')['src']\n",
    "\n",
    "# Attributes\n",
    "attributes = soup.find('div', class_='content-description')\n",
    "attributes = attributes.find_all('p')\n",
    "\n",
    "def attribute_extract(soup):\n",
    "    attributes = soup.find('div', class_='content-description')\n",
    "    attributes = attributes.find_all('p')\n",
    "    last = ''\n",
    "    attributes_list = [i.text.strip() for i in attributes if i.text != None]\n",
    "    idx = 0\n",
    "    for i in attributes_list:\n",
    "        new_elem = re.sub('  ', '', i)\n",
    "        new_elem = re.sub('\\\\n\\\\n\\\\n','___',new_elem)\n",
    "        new_elem = new_elem.split('___')\n",
    "        if len(new_elem) != 1:\n",
    "            new_elem = [re.sub('\\\\n', '', i) for i in new_elem]\n",
    "            attributes_list[idx] = new_elem\n",
    "        else:\n",
    "            new_elem = re.sub('\\\\n',' ',new_elem[0])\n",
    "            attributes_list[idx] = new_elem\n",
    "        idx += 1 \n",
    "    return attributes_list\n",
    "\n",
    "attributes = attribute_extract(soup)\n",
    "location = attributes[0]\n",
    "category = attributes[1]\n",
    "job_type = attributes[2]\n",
    "skills = attributes[3]\n",
    "language = attributes[4]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(company)\n",
    "print(industry)\n",
    "print(followers)\n",
    "print(expiration)\n",
    "print(logo)\n",
    "print(location)\n",
    "print(category)\n",
    "print(skills)\n",
    "print(language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gentofte, Denmark\n",
      "Administration\n",
      "Part-time\n",
      "['Team Player', ' Structured', ' Coordination']\n",
      "English, Professional\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BASE_URL = 'https://graduateland.com/project/45825207/13'\n",
    "BASE_URL = 'https://graduateland.com/job/51447115/13'\n",
    "response = requests.get(BASE_URL)\n",
    "soup = BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "attributes = soup.find('div', class_='content-description')\n",
    "attributes = attributes.find_all('p')\n",
    "\n",
    "last = ''\n",
    "attributes_list = [i.text.strip() for i in attributes if i.text != None]\n",
    "idx = 0\n",
    "for i in attributes_list:\n",
    "    new_elem = re.sub('  ', '', i)\n",
    "    new_elem = re.sub('\\\\n\\\\n\\\\n','___',new_elem)\n",
    "    new_elem = new_elem.split('___')\n",
    "    if len(new_elem) != 1:\n",
    "        new_elem = [re.sub('\\\\n', '', i) for i in new_elem]\n",
    "        attributes_list[idx] = new_elem\n",
    "    else:\n",
    "        new_elem = re.sub('\\\\n',' ',new_elem[0])\n",
    "        attributes_list[idx] = new_elem\n",
    "    idx += 1 \n",
    "#print(attributes_list)\n",
    "location = attributes_list[0]\n",
    "category = attributes_list[1]\n",
    "job_type = attributes_list[2]\n",
    "skills = attributes_list[3]\n",
    "\n",
    "language = attributes_list[4]\n",
    "\n",
    "print(location)\n",
    "print(category)\n",
    "print(job_type)\n",
    "print(skills)\n",
    "print(language)\n",
    "\n",
    "#print(list1)    \n",
    "    \n",
    "#for i in features2:\n",
    "#    print(i.text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "i = i.strip()\n",
    "    for j in i:\n",
    "        print(j)\n",
    "        if (j == ' ' and last == ' ') or (j == 'n' and last == '\\\\'):\n",
    "            continue\n",
    "        else: \n",
    "            new_elem.append(j)\n",
    "        last = j\n",
    "    list1[idx] = ''.join(new_elem)\n",
    "    idx += 1 \n",
    "print(list1)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparekassen Sjælland-Fyn\n",
      "Banking & Finance\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "BASE_URL = 'https://graduateland.com/project/51506904/13'\n",
    "CACHE_DICT = {}\n",
    "\n",
    "headers = {'User-Agent': 'UMSI 507 Course Project - Python Web Scraping','From': 'youremail@domain.com','Course-Info': 'https://www.si.umich.edu/programs/courses/507'}\n",
    "\n",
    "##### Make Soup for courses page\n",
    "\n",
    "\n",
    "\n",
    "def load_cache():\n",
    "    try:\n",
    "        cache_file = open(CACHE_FILE_NAME, 'r')\n",
    "        cache_file_contents = cache_file.read()\n",
    "        cache = json.loads(cache_file_contents)\n",
    "        cache_file.close()\n",
    "    except:\n",
    "        cache = {}\n",
    "    return cache\n",
    "\n",
    "\n",
    "def save_cache(cache):\n",
    "    cache_file = open(CACHE_FILE_NAME, 'w')\n",
    "    contents_to_write = json.dumps(cache)\n",
    "    cache_file.write(contents_to_write)\n",
    "    cache_file.close()\n",
    "\n",
    "\n",
    "def make_url_request_using_cache(url, cache):\n",
    "    if (url in cache.keys()): # the url is our unique key\n",
    "        print(\"Using cache\")\n",
    "        return cache[url]\n",
    "    else:\n",
    "        print(\"Fetching\")\n",
    "        time.sleep(1)\n",
    "        response = requests.get(url, headers=headers)\n",
    "        cache[url] = response.text\n",
    "        save_cache(cache)\n",
    "        return cache[url]\n",
    "\n",
    "\n",
    "# Load the cache, save in global variable\n",
    "CACHE_DICT = load_cache()\n",
    "\n",
    "courses_page_url = BASE_URL + COURSES_PATH\n",
    "url_text = make_url_request_using_cache(courses_page_url, CACHE_DICT)\n",
    "#response = requests.get(courses_page_url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '\\n', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'f', 'o', 'l', 'l', 'o', 'w', 'e', 'r', 's']\n"
     ]
    }
   ],
   "source": [
    "company = soup.find('div', class_='headline').find_all('p')\n",
    "print(list(str(company[1].text.strip())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "\n",
      "Passioneret servicehaj som 1. assistentHvad slår en passioneret servicehaj, der lige hurtigt viser dig vej til ugens spottilbud? Ikke ret meget, hvis du spørger os. Og hvis du er enig, så er denne stilling måske noget for dig. Som 1. assistent finder man dig nemlig overalt i butikken. Her er det dit ansvar, at varehylderne spiller, og at butikken ser præsentabel ud. Der er aldrig to dage, der er ens, og vi regner med dig – ligesom at du altid kan regne med os. Helt kort så skal dualtid levere den bedste kundeserviceekspedere kunder ved kassentrimme hylder og fylde varer oppåtage dig lederansvar, når det kræves og fx lukke butikken de dage, hvor butikschefen ikke er derAlle de gode grundeSom 1. assistent i Netto bliver du en del af en sjov arbejdsplads med fantastiske kollegaer, godt fællesskab og mange forskellige slags opgaver. Og hvis vi lige skal nævne et par andre ting, så får du også frisk frugt og grønt på arbejdspladsen – hver dagfoden ind hos Salling Group og et kæmpe karriereuniversmulighed for at tilføje ledererfaring til dit CVmulighed for leje af feriehuse og lejligheder i ind- og udlandpersonalerabat i Bilka, føtex, Netto, Carl's Jr. og StarbucksLedertypen?Hvis du gerne vil udforske dit ledertalent endnu mere, så er du kommet til det rette sted. Vi er eksperter i at forme fremtidens ledere, og du har mulighed for at komme på målrettede trainee-forløb eller løbende dygtiggøre dig ved at få nye ansvarsområder i butikken. KompetencerDu har gerne erfaring fra en butik eller et andet job med kundeserviceDu trives et sted, hvor der er drøn påMåske har du allerede ledererfaring – men det er ikke noget krav, for vi klær’ dig grundigt påEr du vores nye kollega?Send din ansøgning til os via linket nedenfor. Vi inviterer løbende til samtale, så søg allerede i dag. Vi glæder os til at høre fra dig!Netto – vi bærer vores logo med stolthed  Du kender måske allerede Netto som Danmarks største dagligvarekæde – men vidste du også, at vores medarbejdere er rigtig glade for at arbejde her? Du bliver nemlig del af et helt specielt sammenhold med masser af opbakning, og hvor vi alle løfter i flok. For det, vi laver, er vigtigt, men dem, vi laver det med, er endnu vigtigere. Vi håber, du er lige så begejstret for dyrevelfærd, reduktion af madspild og en bæredygtig fremtid, som vi er. Vi har bl.a. vores eget økologiske brand: ØGO. Og det er naturligvis til gode Netto-priser. Foretrukket startdato: 01/12/2022  Eventuel slutdato:  Adresse: Smakkegårdsvej 114, 2820 Gentofte  Jobtype: Deltid Jobkategori: Salg - Generelt Jobniveau: Medarbejder \n",
      "\n"
     ]
    }
   ],
   "source": [
    "course_desc = soup.find('article', class_='box-item job-content')\n",
    "print(len(course_desc))\n",
    "print(course_desc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_save = course_desc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Passioneret servicehaj som 1. assistentHvad slår en passioneret servicehaj, der lige hurtigt viser dig vej til ugens spottilbud? Ikke ret meget, hvis du spørger os. Og hvis du er enig, så er denne stilling måske noget for dig. Som 1. assistent finder man dig nemlig overalt i butikken. Her er det dit ansvar, at varehylderne spiller, og at butikken ser præsentabel ud. Der er aldrig to dage, der er ens, og vi regner med dig – ligesom at du altid kan regne med os. Helt kort så skal dualtid levere den bedste kundeserviceekspedere kunder ved kassentrimme hylder og fylde varer oppåtage dig lederansvar, når det kræves og fx lukke butikken de dage, hvor butikschefen ikke er derAlle de gode grundeSom 1. assistent i Netto bliver du en del af en sjov arbejdsplads med fantastiske kollegaer, godt fællesskab og mange forskellige slags opgaver. Og hvis vi lige skal nævne et par andre ting, så får du også frisk frugt og grønt på arbejdspladsen – hver dagfoden ind hos Salling Group og et kæmpe karriereuniversmulighed for at tilføje ledererfaring til dit CVmulighed for leje af feriehuse og lejligheder i ind- og udlandpersonalerabat i Bilka, føtex, Netto, Carl's Jr. og StarbucksLedertypen?Hvis du gerne vil udforske dit ledertalent endnu mere, så er du kommet til det rette sted. Vi er eksperter i at forme fremtidens ledere, og du har mulighed for at komme på målrettede trainee-forløb eller løbende dygtiggøre dig ved at få nye ansvarsområder i butikken. KompetencerDu har gerne erfaring fra en butik eller et andet job med kundeserviceDu trives et sted, hvor der er drøn påMåske har du allerede ledererfaring – men det er ikke noget krav, for vi klær’ dig grundigt påEr du vores nye kollega?Send din ansøgning til os via linket nedenfor. Vi inviterer løbende til samtale, så søg allerede i dag. Vi glæder os til at høre fra dig!Netto – vi bærer vores logo med stolthed  Du kender måske allerede Netto som Danmarks største dagligvarekæde – men vidste du også, at vores medarbejdere er rigtig glade for at arbejde her? Du bliver nemlig del af et helt specielt sammenhold med masser af opbakning, og hvor vi alle løfter i flok. For det, vi laver, er vigtigt, men dem, vi laver det med, er endnu vigtigere. Vi håber, du er lige så begejstret for dyrevelfærd, reduktion af madspild og en bæredygtig fremtid, som vi er. Vi har bl.a. vores eget økologiske brand: ØGO. Og det er naturligvis til gode Netto-priser. Foretrukket startdato: 01/12/2022  Eventuel slutdato:  Adresse: Smakkegårdsvej 114, 2820 Gentofte  Jobtype: Deltid Jobkategori: Salg - Generelt Jobniveau: Medarbejder\n"
     ]
    }
   ],
   "source": [
    "BASE_URL = 'https://graduateland.com/job/51498185/23/1-assistent-gentofte'\n",
    "\n",
    "response = requests.get(BASE_URL)\n",
    "soup = BeautifulSoup(response.text,'html.parser')\n",
    "course_desc = soup.find_all('article', class_='box-item job-content')\n",
    "print(len(course_desc))\n",
    "print(course_desc[0].text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "acb118ada14cafd3b3da3ef21dccf95768441697cf5d12d40daa79f849726a2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
